import os
import sys
import time
import sqlite3
import requests
import json
from datetime import datetime, timedelta
from dotenv import load_dotenv
import logging
from typing import Dict, List, Optional, Any

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('polygon_extraction.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Tradeable exchanges (NYSE and NASDAQ variants)
TRADEABLE_EXCHANGES = ['XNYS', 'XNAS', 'XNGS', 'XNCM', 'XNGM']

class PolygonDataExtractor:
    """Extracts NYSE/NASDAQ data from Polygon.io API with daily table organization"""
    
    def __init__(self, api_key: str, db_path: str = None):
        self.api_key = api_key
        self.base_url = "https://api.polygon.io"
        
        # Create data directory if it doesn't exist
        self.data_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data')
        os.makedirs(self.data_dir, exist_ok=True)
        
        # Use provided path or default to data directory
        self.db_path = db_path or os.path.join(self.data_dir, "polygon_market_data.db")
        
        # Get today's date for table suffixes
        self.date_suffix = datetime.now().strftime('%Y%m%d')
        
        self.log_file = 'polygon_extraction.log'
        self.session = requests.Session()
        self.session.headers.update({
            'Authorization': f'Bearer {api_key}'
        })
        
        # Track statistics
        self.stats = {
            'api_calls': 0,
            'total_records': 0,
            'start_time': None,
            'errors': 0
        }
        
        # Update .gitignore
        self.update_gitignore()
        
        # Initialize database
        self.init_database()
    
    def update_gitignore(self):
        """Add generated files to .gitignore"""
        gitignore_entries = [
            # Data directory
            'data/',
            'data/*.db',
            'data/*.db-journal',
            'data/*.sqlite',
            'data/*.sqlite3',
            
            # Database files
            '*.db',
            '*.db-journal',
            '*.sqlite',
            '*.sqlite3',
            'polygon_complete_data.db',
            'polygon_market_data.db',
            
            # Log files
            '*.log',
            'polygon_extraction.log',
            'polygon_data_collection.log',
            
            # Backup files
            '*.db-backup',
            '*.bak',
            
            # Environment files
            '.env',
            '.env.*',
            '*.env',
            
            # Python cache
            '__pycache__/',
            '*.pyc',
            '*.pyo',
            '*.pyd',
            '.Python',
            
            # IDE files
            '.vscode/',
            '.idea/',
            '*.swp',
            '*.swo',
            '*~',
            
            # OS files
            '.DS_Store',
            'Thumbs.db',
            'desktop.ini'
        ]
        
        # Read existing .gitignore
        existing_entries = set()
        gitignore_path = '.gitignore'
        
        try:
            if os.path.exists(gitignore_path):
                with open(gitignore_path, 'r') as f:
                    existing_entries = set(line.strip() for line in f if line.strip() and not line.startswith('#'))
        except Exception as e:
            logger.warning(f"Could not read existing .gitignore: {e}")
        
        # Find new entries to add
        new_entries = []
        for entry in gitignore_entries:
            if entry not in existing_entries:
                new_entries.append(entry)
        
        # Append new entries if any
        if new_entries:
            try:
                with open(gitignore_path, 'a') as f:
                    # Add header if file is new or empty
                    if not existing_entries:
                        f.write("# Polygon Data Extractor - Auto-generated entries\n")
                        f.write("# Generated by polygon_extractor.py\n\n")
                    else:
                        f.write("\n# Polygon Data Extractor files\n")
                    
                    for entry in new_entries:
                        f.write(f"{entry}\n")
                
                logger.info(f"✅ Added {len(new_entries)} entries to .gitignore")
                
            except Exception as e:
                logger.warning(f"Could not update .gitignore: {e}")
        else:
            logger.info("✅ .gitignore already up to date")
    
    def init_database(self):
        """Create all necessary tables with date suffixes"""
        logger.info(f"Initializing SQLite database with tables for {self.date_suffix}...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Enable foreign keys
        cursor.execute("PRAGMA foreign_keys = ON")
        
        # Master tickers table (no date suffix - this is reference data)
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS tickers (
            ticker TEXT PRIMARY KEY,
            name TEXT,
            market TEXT,
            locale TEXT,
            primary_exchange TEXT,
            type TEXT,
            active BOOLEAN,
            currency_name TEXT,
            cik TEXT,
            composite_figi TEXT,
            share_class_figi TEXT,
            last_updated_utc TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """)
        
        # Daily snapshots table
        cursor.execute(f"""
        CREATE TABLE IF NOT EXISTS snapshots_{self.date_suffix} (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            ticker TEXT,
            timestamp INTEGER,
            open REAL,
            high REAL,
            low REAL,
            close REAL,
            volume INTEGER,
            vwap REAL,
            prev_open REAL,
            prev_high REAL,
            prev_low REAL,
            prev_close REAL,
            prev_volume INTEGER,
            prev_vwap REAL,
            change REAL,
            change_percent REAL,
            last_trade_price REAL,
            last_trade_size INTEGER,
            last_trade_conditions TEXT,
            last_quote_bid REAL,
            last_quote_ask REAL,
            last_quote_bid_size INTEGER,
            last_quote_ask_size INTEGER,
            updated_at INTEGER,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(ticker, timestamp)
        )
        """)
        
        # Daily news table
        cursor.execute(f"""
        CREATE TABLE IF NOT EXISTS news_{self.date_suffix} (
            id TEXT PRIMARY KEY,
            publisher_name TEXT,
            publisher_homepage TEXT,
            publisher_logo TEXT,
            publisher_favicon TEXT,
            title TEXT,
            author TEXT,
            published_utc TEXT,
            article_url TEXT,
            tickers TEXT,  -- JSON array
            image_url TEXT,
            description TEXT,
            keywords TEXT,  -- JSON array
            amp_url TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """)
        
        # Daily news tickers junction table
        cursor.execute(f"""
        CREATE TABLE IF NOT EXISTS news_tickers_{self.date_suffix} (
            news_id TEXT,
            ticker TEXT,
            PRIMARY KEY (news_id, ticker),
            FOREIGN KEY (news_id) REFERENCES news_{self.date_suffix}(id),
            FOREIGN KEY (ticker) REFERENCES tickers(ticker)
        )
        """)
        
        # Daily trades table
        cursor.execute(f"""
        CREATE TABLE IF NOT EXISTS trades_{self.date_suffix} (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            ticker TEXT,
            timestamp INTEGER,
            participant_timestamp INTEGER,
            price REAL,
            size INTEGER,
            conditions TEXT,  -- JSON array
            exchange INTEGER,
            trf_id INTEGER,
            sequence_number INTEGER,
            sip_timestamp INTEGER,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(ticker, sip_timestamp, sequence_number)
        )
        """)
        
        # API call log (no date suffix - keeps full history)
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS api_calls (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            date TEXT,
            endpoint TEXT,
            params TEXT,
            records_fetched INTEGER,
            response_time REAL,
            status_code INTEGER,
            error_message TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """)
        
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS sentiment_analysis (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        ticker TEXT,
        company_name TEXT,
        analysis_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        model_used TEXT,
        total_articles INTEGER,
        sentiment TEXT,
        sentiment_score REAL,
        sentiment_distribution TEXT,
        confidence_scores TEXT,
        key_insights TEXT,
        recent_headlines TEXT,
        summary TEXT,
        FOREIGN KEY (ticker) REFERENCES tickers(ticker)
        )
        """)
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_sentiment_analysis_ticker ON sentiment_analysis(ticker)")
        
        # Create indexes for daily tables
        cursor.execute(f"CREATE INDEX IF NOT EXISTS idx_snapshots_{self.date_suffix}_ticker ON snapshots_{self.date_suffix}(ticker)")
        cursor.execute(f"CREATE INDEX IF NOT EXISTS idx_snapshots_{self.date_suffix}_timestamp ON snapshots_{self.date_suffix}(timestamp)")
        cursor.execute(f"CREATE INDEX IF NOT EXISTS idx_news_{self.date_suffix}_published ON news_{self.date_suffix}(published_utc)")
        cursor.execute(f"CREATE INDEX IF NOT EXISTS idx_trades_{self.date_suffix}_ticker ON trades_{self.date_suffix}(ticker)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_api_calls_date ON api_calls(date)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_tickers_exchange ON tickers(primary_exchange)")
        
        conn.commit()
        conn.close()
        logger.info("Database initialized successfully")
    
    def log_api_call(self, endpoint: str, params: Dict, records: int, response_time: float, 
                     status_code: int, error: Optional[str] = None):
        """Log API call details"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
        INSERT INTO api_calls (date, endpoint, params, records_fetched, response_time, status_code, error_message)
        VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (self.date_suffix, endpoint, json.dumps(params), records, response_time, status_code, error))
        
        conn.commit()
        conn.close()
    
    def make_paginated_request(self, endpoint: str, params: Dict) -> List[Dict]:
        """Make paginated API requests and return all results"""
        all_results = []
        next_url = None
        page = 0
        
        while True:
            page += 1
            start_time = time.time()
            
            try:
                if next_url:
                    response = self.session.get(next_url)
                else:
                    url = f"{self.base_url}{endpoint}"
                    response = self.session.get(url, params=params)
                
                response_time = time.time() - start_time
                self.stats['api_calls'] += 1
                
                if response.status_code == 200:
                    data = response.json()
                    results = data.get('results', [])
                    all_results.extend(results)
                    
                    logger.info(f"Page {page}: Fetched {len(results)} records from {endpoint}")
                    self.log_api_call(endpoint, params, len(results), response_time, response.status_code)
                    
                    # Check for next page
                    next_url = data.get('next_url')
                    if not next_url or len(results) == 0:
                        break
                    
                    # Rate limiting - be respectful
                    time.sleep(0.2)
                else:
                    error_msg = f"API error: {response.status_code} - {response.text}"
                    logger.error(error_msg)
                    self.log_api_call(endpoint, params, 0, response_time, response.status_code, error_msg)
                    self.stats['errors'] += 1
                    break
                    
            except Exception as e:
                error_msg = f"Request error: {str(e)}"
                logger.error(error_msg)
                self.log_api_call(endpoint, params, 0, time.time() - start_time, 0, error_msg)
                self.stats['errors'] += 1
                break
        
        logger.info(f"Total records fetched from {endpoint}: {len(all_results)}")
        return all_results
    
    def extract_all_tickers(self, market: str = 'stocks', ticker_type: str = 'CS'):
        """Extract all tickers from NYSE/NASDAQ exchanges"""
        logger.info(f"Extracting {market} tickers from NYSE/NASDAQ...")
        
        params = {
            'market': market,
            'active': 'true',
            'type': ticker_type,  # CS = Common Stock
            'limit': 1000,
            'order': 'asc',
            'sort': 'ticker',
            'apiKey': self.api_key
        }
        
        tickers = self.make_paginated_request('/v3/reference/tickers', params)
        
        # Save to database
        if tickers:
            self._save_tickers(tickers)
            self.stats['total_records'] += len(tickers)
        
        return tickers
    
    def _save_tickers(self, tickers: List[Dict]):
        """Save tickers to database (only NYSE/NASDAQ)"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        saved_count = 0
        for ticker in tickers:
            # Only save if from tradeable exchange
            if ticker.get('primary_exchange') in TRADEABLE_EXCHANGES:
                cursor.execute("""
                INSERT OR REPLACE INTO tickers 
                (ticker, name, market, locale, primary_exchange, type, active, 
                 currency_name, cik, composite_figi, share_class_figi, last_updated_utc)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    ticker.get('ticker'),
                    ticker.get('name'),
                    ticker.get('market'),
                    ticker.get('locale'),
                    ticker.get('primary_exchange'),
                    ticker.get('type'),
                    ticker.get('active'),
                    ticker.get('currency_name'),
                    ticker.get('cik'),
                    ticker.get('composite_figi'),
                    ticker.get('share_class_figi'),
                    ticker.get('last_updated_utc')
                ))
                saved_count += 1
        
        conn.commit()
        conn.close()
        logger.info(f"Saved {saved_count} NYSE/NASDAQ tickers (filtered from {len(tickers)} total)")
    
    def extract_all_snapshots(self, include_otc: bool = False):
        """Extract full market snapshot"""
        logger.info("Extracting market snapshots...")
        
        params = {
            'include_otc': str(include_otc).lower(),
            'apiKey': self.api_key
        }
        
        url = f"{self.base_url}/v2/snapshot/locale/us/markets/stocks/tickers"
        
        start_time = time.time()
        try:
            response = self.session.get(url, params=params)
            response_time = time.time() - start_time
            self.stats['api_calls'] += 1
            
            if response.status_code == 200:
                data = response.json()
                tickers_data = data.get('tickers', [])
                
                logger.info(f"Fetched {len(tickers_data)} snapshots")
                self.log_api_call('/v2/snapshot/locale/us/markets/stocks/tickers', 
                                params, len(tickers_data), response_time, response.status_code)
                
                if tickers_data:
                    self._save_snapshots(tickers_data)
                    self.stats['total_records'] += len(tickers_data)
                
                return tickers_data
            else:
                error_msg = f"Snapshot API error: {response.status_code}"
                logger.error(error_msg)
                self.log_api_call('/v2/snapshot/locale/us/markets/stocks/tickers', 
                                params, 0, response_time, response.status_code, error_msg)
                self.stats['errors'] += 1
                return []
                
        except Exception as e:
            error_msg = f"Snapshot request error: {str(e)}"
            logger.error(error_msg)
            self.stats['errors'] += 1
            return []
    
    def _save_snapshots(self, snapshots: List[Dict]):
        """Save snapshots to date-specific table (only for NYSE/NASDAQ tickers)"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Get list of valid tickers
        cursor.execute("SELECT ticker FROM tickers WHERE primary_exchange IN ({})".format(
            ','.join(['?'] * len(TRADEABLE_EXCHANGES))
        ), TRADEABLE_EXCHANGES)
        valid_tickers = set(row[0] for row in cursor.fetchall())
        
        saved_count = 0
        for snap in snapshots:
            # Only save if ticker is in our valid list
            if snap.get('ticker') in valid_tickers:
                # Extract nested data
                day = snap.get('day', {})
                prev_day = snap.get('prevDay', {})
                last_trade = snap.get('lastTrade', {})
                last_quote = snap.get('lastQuote', {})
                
                cursor.execute(f"""
                INSERT OR REPLACE INTO snapshots_{self.date_suffix}
                (ticker, timestamp, open, high, low, close, volume, vwap,
                 prev_open, prev_high, prev_low, prev_close, prev_volume, prev_vwap,
                 change, change_percent, last_trade_price, last_trade_size,
                 last_quote_bid, last_quote_ask, last_quote_bid_size, last_quote_ask_size,
                 updated_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    snap.get('ticker'),
                    int(time.time() * 1000),  # Current timestamp in ms
                    day.get('o'), day.get('h'), day.get('l'), day.get('c'),
                    day.get('v'), day.get('vw'),
                    prev_day.get('o'), prev_day.get('h'), prev_day.get('l'), prev_day.get('c'),
                    prev_day.get('v'), prev_day.get('vw'),
                    snap.get('todaysChange'), snap.get('todaysChangePerc'),
                    last_trade.get('p') if last_trade else None,
                    last_trade.get('s') if last_trade else None,
                    last_quote.get('p') if last_quote else None,
                    last_quote.get('P') if last_quote else None,
                    last_quote.get('s') if last_quote else None,
                    last_quote.get('S') if last_quote else None,
                    snap.get('updated')
                ))
                saved_count += 1
        
        conn.commit()
        conn.close()
        logger.info(f"Saved {saved_count} NYSE/NASDAQ snapshots (filtered from {len(snapshots)} total)")
    
    def extract_all_news(self, limit_per_page: int = 1000, max_pages: int = 10):
        """Extract news articles with pagination"""
        logger.info("Extracting news articles...")
        
        params = {
            'limit': limit_per_page,
            'order': 'desc',
            'sort': 'published_utc',
            'apiKey': self.api_key
        }
        
        news = self.make_paginated_request('/v2/reference/news', params)
        
        # Debug logging
        logger.info(f"📰 Total news articles fetched from API: {len(news)}")
        if news and len(news) > 0:
            # Sample first few articles to see what tickers they mention
            sample_size = min(5, len(news))
            logger.info(f"Sample of first {sample_size} articles:")
            for i, article in enumerate(news[:sample_size]):
                logger.info(f"  Article {i+1}: {article.get('title', 'No title')[:50]}...")
                logger.info(f"    Tickers: {article.get('tickers', [])}")
        
        if news:
            self._save_news(news)
            self.stats['total_records'] += len(news)
        
        return news

    def _save_news(self, news_items: List[Dict]):
        """Save news to date-specific table (only for NYSE/NASDAQ tickers)"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # First, check if we have any tickers at all
        cursor.execute("SELECT COUNT(*) FROM tickers")
        total_tickers = cursor.fetchone()[0]
        logger.info(f"📊 Total tickers in database: {total_tickers}")
        
        # Get list of valid tickers
        cursor.execute("SELECT ticker FROM tickers WHERE primary_exchange IN ({})".format(
            ','.join(['?'] * len(TRADEABLE_EXCHANGES))
        ), TRADEABLE_EXCHANGES)
        valid_tickers = set(row[0] for row in cursor.fetchall())
        
        logger.info(f"📊 Valid NYSE/NASDAQ tickers found: {len(valid_tickers)}")
        
        # Show sample of valid tickers for debugging
        if valid_tickers:
            sample_tickers = list(valid_tickers)[:10]
            logger.info(f"Sample valid tickers: {sample_tickers}")
        
        saved_count = 0
        filtered_count = 0
        no_ticker_count = 0
        
        for article in news_items:
            # Check if article has tickers
            article_tickers = article.get('tickers', [])
            
            if not article_tickers:
                no_ticker_count += 1
                continue
            
            # Find valid tickers in this article
            valid_article_tickers = [t for t in article_tickers if t in valid_tickers]
            
            # Debug logging for filtered articles
            if not valid_article_tickers:
                filtered_count += 1
                if filtered_count <= 5:  # Log first 5 filtered articles
                    logger.debug(f"Filtered article: '{article.get('title', 'No title')[:50]}...'")
                    logger.debug(f"  Had tickers: {article_tickers} (none are NYSE/NASDAQ)")
                continue
            
            # Save article since it has valid tickers
            publisher = article.get('publisher', {})
            
            try:
                cursor.execute(f"""
                INSERT OR IGNORE INTO news_{self.date_suffix}
                (id, publisher_name, publisher_homepage, publisher_logo, publisher_favicon,
                 title, author, published_utc, article_url, tickers, image_url,
                 description, keywords, amp_url)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    article.get('id'),
                    publisher.get('name'),
                    publisher.get('homepage_url'),
                    publisher.get('logo_url'),
                    publisher.get('favicon_url'),
                    article.get('title'),
                    article.get('author'),
                    article.get('published_utc'),
                    article.get('article_url'),
                    json.dumps(valid_article_tickers),  # Only save valid tickers
                    article.get('image_url'),
                    article.get('description'),
                    json.dumps(article.get('keywords', [])),
                    article.get('amp_url')
                ))
                
                # Save ticker relationships
                for ticker in valid_article_tickers:
                    cursor.execute(f"""
                    INSERT OR IGNORE INTO news_tickers_{self.date_suffix} (news_id, ticker)
                    VALUES (?, ?)
                    """, (article.get('id'), ticker))
                
                saved_count += 1
                
            except Exception as e:
                logger.error(f"Error saving article: {str(e)}")
                logger.error(f"Article ID: {article.get('id')}")
        
        conn.commit()
        conn.close()
        
        # Detailed summary
        logger.info(f"📰 News Processing Summary:")
        logger.info(f"  - Total articles processed: {len(news_items)}")
        logger.info(f"  - Articles with no tickers: {no_ticker_count}")
        logger.info(f"  - Articles filtered (non-NYSE/NASDAQ): {filtered_count}")
        logger.info(f"  - Articles saved (NYSE/NASDAQ): {saved_count}")
    
    def extract_sample_trades(self, ticker: str = 'AAPL', date: Optional[str] = None):
        """Extract sample trades for a ticker (limited by API)"""
        logger.info(f"Extracting sample trades for {ticker}...")
        
        if not date:
            date = datetime.now().strftime('%Y-%m-%d')
        
        params = {
            'timestamp': date,
            'limit': 50000,  # Max limit
            'apiKey': self.api_key
        }
        
        trades = self.make_paginated_request(f'/v3/trades/{ticker}', params)
        
        if trades:
            self._save_trades(trades, ticker)
            self.stats['total_records'] += len(trades)
        
        return trades
    
    def _save_trades(self, trades: List[Dict], ticker: str):
        """Save trades to date-specific table"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for trade in trades:
            cursor.execute(f"""
            INSERT OR IGNORE INTO trades_{self.date_suffix}
            (ticker, timestamp, participant_timestamp, price, size, conditions,
             exchange, trf_id, sequence_number, sip_timestamp)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                ticker,
                trade.get('timestamp'),
                trade.get('participant_timestamp'),
                trade.get('price'),
                trade.get('size'),
                json.dumps(trade.get('conditions', [])),
                trade.get('exchange'),
                trade.get('trf_id'),
                trade.get('sequence_number'),
                trade.get('sip_timestamp')
            ))
        
        conn.commit()
        conn.close()
        logger.info(f"Saved {len(trades)} trades to database")
    
    def get_available_dates(self):
        """Get list of dates with available data"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Get all snapshot tables
        cursor.execute("""
        SELECT name FROM sqlite_master 
        WHERE type='table' AND name LIKE 'snapshots_%'
        ORDER BY name
        """)
        
        tables = cursor.fetchall()
        dates = [table[0].replace('snapshots_', '') for table in tables]
        
        conn.close()
        return dates
    
    def query_historical_data(self, ticker: str, start_date: str, end_date: str):
        """Query data across multiple daily tables"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Convert dates to YYYYMMDD format
        start = datetime.strptime(start_date, '%Y-%m-%d').strftime('%Y%m%d')
        end = datetime.strptime(end_date, '%Y-%m-%d').strftime('%Y%m%d')
        
        # Get relevant tables
        available_dates = self.get_available_dates()
        relevant_dates = [d for d in available_dates if start <= d <= end]
        
        # Union query across all relevant tables
        union_parts = []
        for date in relevant_dates:
            union_parts.append(f"""
            SELECT *, '{date}' as data_date 
            FROM snapshots_{date} 
            WHERE ticker = ?
            """)
        
        if union_parts:
            query = " UNION ALL ".join(union_parts) + " ORDER BY data_date"
            cursor.execute(query, [ticker] * len(relevant_dates))
            results = cursor.fetchall()
        else:
            results = []
        
        conn.close()
        return results
    
    def run_complete_extraction(self):
        """Run the complete extraction process"""
        self.stats['start_time'] = time.time()
        
        logger.info("=" * 60)
        logger.info("Starting Polygon.io NYSE/NASDAQ Data Extraction")
        logger.info(f"Date: {self.date_suffix}")
        logger.info("=" * 60)
        
        # 1. Extract all tickers (only common stocks)
        logger.info("\n📊 PHASE 1: Extracting NYSE/NASDAQ Tickers")
        tickers = self.extract_all_tickers(market='stocks', ticker_type='CS')
        logger.info(f"✅ Extracted {len(tickers)} stock tickers from API")
        
        # Verify tickers were saved to database
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("SELECT COUNT(*) FROM tickers")
        total_in_db = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM tickers WHERE primary_exchange IN ({})".format(
            ','.join(['?'] * len(TRADEABLE_EXCHANGES))
        ), TRADEABLE_EXCHANGES)
        nyse_nasdaq_in_db = cursor.fetchone()[0]
        
        conn.close()
        
        logger.info(f"✅ Database verification:")
        logger.info(f"   - Total tickers in DB: {total_in_db}")
        logger.info(f"   - NYSE/NASDAQ tickers in DB: {nyse_nasdaq_in_db}")
        
        if nyse_nasdaq_in_db == 0:
            logger.error("❌ No NYSE/NASDAQ tickers found in database! News filtering will fail.")
            logger.error("   Check if ticker extraction is working correctly.")
            return
        
        # 2. Extract market snapshots
        logger.info("\n📸 PHASE 2: Extracting Market Snapshots")
        snapshots = self.extract_all_snapshots(include_otc=False)
        logger.info(f"✅ Extracted {len(snapshots)} market snapshots")
        
        # 3. Extract news
        logger.info("\n📰 PHASE 3: Extracting News Articles")
        news = self.extract_all_news(limit_per_page=1000, max_pages=5)
        logger.info(f"✅ Extracted {len(news)} news articles")
        
        # 4. Extract sample trades for top NYSE/NASDAQ tickers
        logger.info("\n💹 PHASE 4: Extracting Sample Trades")
        
        # Get top tickers by volume from today's snapshots
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute(f"""
        SELECT ticker 
        FROM snapshots_{self.date_suffix} 
        WHERE volume IS NOT NULL 
        ORDER BY volume DESC 
        LIMIT 5
        """)
        top_tickers = [row[0] for row in cursor.fetchall()]
        conn.close()
        
        if not top_tickers:
            top_tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA']
            
        for ticker in top_tickers:
            trades = self.extract_sample_trades(ticker)
            logger.info(f"✅ Extracted {len(trades)} trades for {ticker}")
            time.sleep(1)  # Rate limiting
        
        # Calculate final statistics
        elapsed_time = time.time() - self.stats['start_time']
        
        logger.info("\n" + "=" * 60)
        logger.info("EXTRACTION COMPLETE - SUMMARY")
        logger.info("=" * 60)
        logger.info(f"Date: {self.date_suffix}")
        logger.info(f"Total API Calls: {self.stats['api_calls']}")
        logger.info(f"Total Records Extracted: {self.stats['total_records']:,}")
        logger.info(f"Total Errors: {self.stats['errors']}")
        logger.info(f"Total Time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)")
        if self.stats['api_calls'] > 0:
            logger.info(f"Average Time per API Call: {elapsed_time/self.stats['api_calls']:.2f} seconds")
        logger.info(f"Database Size: {os.path.getsize(self.db_path) / 1024 / 1024:.2f} MB")
        logger.info(f"Database Path: {self.db_path}")
        
        # Generate database summary
        self._generate_db_summary()
    
    def _generate_db_summary(self):
        """Generate summary of data in database"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        logger.info("\n📊 DATABASE SUMMARY:")
        
        # Master tables
        logger.info("\nMaster Tables:")
        cursor.execute("SELECT COUNT(*) FROM tickers")
        count = cursor.fetchone()[0]
        logger.info(f"  tickers: {count:,} records")
        
        cursor.execute("SELECT COUNT(*) FROM tickers WHERE primary_exchange IN ({})".format(
            ','.join(['?'] * len(TRADEABLE_EXCHANGES))
        ), TRADEABLE_EXCHANGES)
        nyse_nasdaq_count = cursor.fetchone()[0]
        logger.info(f"  NYSE/NASDAQ tickers: {nyse_nasdaq_count:,} records")
        
        # Daily tables
        logger.info(f"\nDaily Tables for {self.date_suffix}:")
        tables = [
            (f'snapshots_{self.date_suffix}', 'snapshots'),
            (f'news_{self.date_suffix}', 'news articles'),
            (f'trades_{self.date_suffix}', 'trades')
        ]
        
        for table, desc in tables:
            try:
                cursor.execute(f"SELECT COUNT(*) FROM {table}")
                count = cursor.fetchone()[0]
                logger.info(f"  {desc}: {count:,} records")
            except:
                logger.info(f"  {desc}: table not found")
        
        # Historical data summary
        available_dates = self.get_available_dates()
        logger.info(f"\nHistorical Data Available: {len(available_dates)} days")
        if available_dates:
            logger.info(f"  Date Range: {min(available_dates)} to {max(available_dates)}")
        
        conn.close()


def main():
    """Main function"""
    # Get API key
    API_KEY = os.getenv('API_KEY')
    if not API_KEY:
        logger.error("❌ API_KEY not found in .env file!")
        sys.exit(1)
    
    # Create extractor
    extractor = PolygonDataExtractor(API_KEY)
    
    # Run complete extraction
    try:
        extractor.run_complete_extraction()
        
        logger.info(f"\n✅ All NYSE/NASDAQ data successfully extracted and stored in:")
        logger.info(f"   {extractor.db_path}")
        logger.info("\nYou can query the database using any SQLite client.")
        logger.info(f"Today's data is in tables suffixed with _{extractor.date_suffix}")
        
    except KeyboardInterrupt:
        logger.info("\n⚠️ Extraction interrupted by user")
    except Exception as e:
        logger.error(f"\n❌ Extraction failed: {str(e)}", exc_info=True)


if __name__ == "__main__":
    main()